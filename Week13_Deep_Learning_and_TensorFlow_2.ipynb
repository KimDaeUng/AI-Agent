{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "name": "Week13_Deep Learning and TensorFlow 2.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KimDaeUng/AI-Agent/blob/master/Week13_Deep_Learning_and_TensorFlow_2.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCCk8_dHpuNf",
        "colab_type": "text"
      },
      "source": [
        "# Week13 Deep Learning and TensorFlow 2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "teh54kPruHeI",
        "colab_type": "text"
      },
      "source": [
        "# 1. Convolutional Neural Netowks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fVjs0VLWvP96",
        "colab_type": "text"
      },
      "source": [
        "## Image Classification with CNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4BlGemCK1bdj",
        "colab_type": "text"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mthideHn1bPR",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z8Ncqy7jvenD",
        "colab_type": "text"
      },
      "source": [
        "### Load and Preprocess the Fashion-MNIST Dataset "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-fyQZJQSuKBP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "fashion_mnist = tf.keras.datasets.fashion_mnist\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = fashion_mnist.load_data()\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ziqAmALKuRxL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_train, X_test = X_train / 255.0, X_test / 255.0\n",
        "num_classes = 10\n",
        "\n",
        "X_train = X_train.reshape(X_train.shape[0], 28, 28, 1) # Expand dimension (Channel dim)\n",
        "X_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "heYH1Ssdu67x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "X_test = X_test.reshape(X_test.shape[0], 28, 28, 1)\n",
        "X_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QfbdIbV2u7fm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train = y_train.reshape(y_train.shape[0], 1) \n",
        "y_train.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fILY-2KMu75I",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_test = y_test.reshape(y_test.shape[0], 1) \n",
        "y_test.shape"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Rxz6wkKvHGT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "y_train"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZiVGDjjgvLol",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WZW5kLyWv2UC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = tf.keras.layers\n",
        "base_model = tf.keras.Sequential([\n",
        "layers.Conv2D(32, kernel_size=(3, 3), padding = \"same\", input_shape = (28, 28, 1), \n",
        "                       activation=\"relu\"),\n",
        "layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "layers.MaxPool2D(pool_size=2),\n",
        "layers.Flatten(),\n",
        "layers.Dense(128, activation=\"relu\"),\n",
        "layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rB3FZusjwGJk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer='adam',\n",
        "                   metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VM5sHFb6wpEf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_history = base_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "55pMLSXEwzzk",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.evaluate(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5jlhgreNw3Cr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "base_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9-yYfjONzBjA",
        "colab_type": "text"
      },
      "source": [
        "## Dropout regularization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Pn1fV8iezTiv",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Dropout Model\n",
        "dropout_model = tf.keras.Sequential([\n",
        "tf.keras.layers.Conv2D(32, kernel_size=(3, 3), padding = \"same\", input_shape = (28, 28, 1), \n",
        "                       activation=\"relu\"),\n",
        "tf.keras.layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "tf.keras.layers.MaxPool2D(pool_size=2),\n",
        "\n",
        "tf.keras.layers.Flatten(),\n",
        "tf.keras.layers.Dense(128, activation=\"relu\"),\n",
        "tf.keras.layers.Dropout(0.5),\n",
        "tf.keras.layers.Dense(10, activation=\"softmax\")\n",
        "])\n",
        "\n",
        "dropout_model.compile(loss='sparse_categorical_crossentropy',\n",
        "                   optimizer='adam',\n",
        "                   metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0C22AQ_nzZG8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "drop_history = dropout_model.fit(X_train, y_train, epochs=10, validation_data=(X_test, y_test))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qFOmpzfN31yY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout_model.evaluate(X_train, y_train)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TtOcXvTK33gj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "dropout_model.evaluate(X_test, y_test)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAT_RVIARomJ",
        "colab_type": "text"
      },
      "source": [
        "### Plotting the learning curve"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UuzP0HSfyGw8",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(histories, key='loss'):\n",
        "    plt.figure(figsize=(16,10))\n",
        "\n",
        "    for name, history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                       '--', label=name.title()+' Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "                 label=name.title()+' Train')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(key.replace('_',' ').title())\n",
        "    plt.legend()\n",
        "    plt.xlim([0,max(history.epoch)])\n",
        "\n",
        "plot_history([('Base CNNs', base_history),\n",
        "              ('Dropout CNNs', drop_history)])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ovpZyIhNIgoq"
      },
      "source": [
        "# 2. Recurrent Neural Networks"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "l6bvTAJB1mvT",
        "colab_type": "text"
      },
      "source": [
        "## Character-level Language Model with RNNs"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WGyKZj3bzf9p"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "yG_n40gFzf9s",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "EHDoRoc5PKWz"
      },
      "source": [
        "### Load and preprocess the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pD_55cOxLkAb",
        "colab": {}
      },
      "source": [
        "# 1. Download the Shakespeare's Sonnet dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
        "'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Load whole text file as a string, then decode.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PXSyfmOoWAcM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Take a look first 250 characters\n",
        "print(text[:250])\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(\"=\"*90)\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "IalZLbvOzf-F",
        "colab": {}
      },
      "source": [
        "# 2. Vectorize the text\n",
        "# Creating a mapping from unique characters to indices, and vice versa\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Convert the characters to the indices\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "0UHJDA39zf-O",
        "colab": {}
      },
      "source": [
        "# 3. Creating training task\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "l4hkDU3i7ozi",
        "colab": {}
      },
      "source": [
        "# 'batch' method convert these individual characters to sequences of the desired size\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9NGu-FkO_kYU",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    # input text is shifted to form the target text \n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# 'map' method lets us easily apply a simple function to each batch\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Print the examples \n",
        "for input_ex, target_ex in  dataset.take(1):\n",
        "  print ('Input : ', repr(''.join(idx2char[input_ex.numpy()])))\n",
        "  print ('Target :', repr(''.join(idx2char[target_ex.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "p2pGotuNzf-S",
        "colab": {}
      },
      "source": [
        "# 4. Create training batches\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "r6oUuElIMgVx"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "m8gPwEjRzf-Z"
      },
      "source": [
        "- 3 layers are used to define this model\n",
        "    1. `tf.keras.layers.Embedding`: The input layer, A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "    2. `tf.keras.layers.RNN`: A RNN with size `units=rnn_units`\n",
        "    3. `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zHT8cLh7EAsg",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "MtCrdfzEI2N0",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    layers = tf.keras.layers\n",
        "    model = tf.keras.Sequential([\n",
        "            layers.Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            batch_input_shape=[batch_size, None]\n",
        "                            ),\n",
        "            layers.SimpleRNN(rnn_units, # you can change the RNN type to LSTM or GRU\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform' # Xavier Initialization\n",
        "                            ),\n",
        "            # If you want to build multi-layer RNN, just stack the RNN layer here\n",
        "            # layers.SimpleRNN(rnn_units,\n",
        "            #     return_sequences=True,\n",
        "            #     stateful=True,\n",
        "            #     recurrent_initializer='glorot_uniform' # Xavier Initialization\n",
        "            #     ),\n",
        "            layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "baOROwVlSFYD",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "-ubPo0_9Prjb"
      },
      "source": [
        "### Try the model\n",
        "Before training the model, take a look about how does the model works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "C-_70kKAPrPU",
        "colab": {}
      },
      "source": [
        "# Check the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "vPGmAAXmVLGC",
        "colab": {}
      },
      "source": [
        "# Check the model architecture\n",
        "# Model can be run on inputs of any length \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4V4MfFg0RQJg",
        "colab": {}
      },
      "source": [
        "# We need to sample from the output distribution, not to take the argmax of the distribution\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "# Predictions of the next character index \n",
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "xWcFwPwLSo05",
        "colab": {}
      },
      "source": [
        "# Decode the predictions, the model shows poor performance \n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "LJL0Q0YPY6Ee"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "4HrXTACTdzY-",
        "colab": {}
      },
      "source": [
        "# 1. Attach an optimizer, and a loss function\n",
        "# define the loss function \n",
        "def loss(labels, logits):\n",
        "    # Because the output of model is logit, \n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Test the loss function\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "# Configure the training procedure\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "model.compile(optimizer=optimizer, loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "W6fWTriUZP-n",
        "colab": {}
      },
      "source": [
        "# 2. Configure the checkpoints\n",
        "# `tf.keras.callbacks.ModelCheckpoint` : The callback function to save the model checkpoint\n",
        "\n",
        "# Directory where the model weights will be saved\n",
        "ckpt_dir = './training_rnns_ckpts'\n",
        "\n",
        "# Checkpoint name\n",
        "ckpt_prefix = os.path.join(ckpt_dir, \"ckpt_rnns_{epoch}\")\n",
        "\n",
        "# Callback function to save the model weights\n",
        "ckpt_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=ckpt_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "UK-hmKjYVoll",
        "colab": {}
      },
      "source": [
        "# 3. Execute the training\n",
        "EPOCHS=10\n",
        "rnn_history = model.fit(dataset, epochs=EPOCHS, callbacks=[ckpt_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "kKkD5M6eoSiN"
      },
      "source": [
        "### Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "zk2WJ2-XjkGz",
        "colab": {}
      },
      "source": [
        "# Check the latest checkpoint\n",
        "tf.train.latest_checkpoint(ckpt_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "LycQ-ot_jjyu",
        "colab": {}
      },
      "source": [
        "# To run the model with one sample(not with batch_size of samples),\n",
        "# We rebuild the model, and load the weights from the saved checkpoint. \n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(ckpt_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LLnghlJemP_h",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the model summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "WvuwZBX5Ogfd",
        "colab": {}
      },
      "source": [
        "# The prediction loop\n",
        "def generate_text(model, start_string):\n",
        "  # Number of characters to generate\n",
        "  n_generate = 1000\n",
        "\n",
        "  # Converting start_strings to index (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Making the empty list to store results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures -> more predictable text.\n",
        "  # Higher temperatures -> more surprising text.\n",
        "  temperature = 1\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(n_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Passing the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "ktovv0RFhrkn",
        "colab": {}
      },
      "source": [
        "# It shows poor performance \n",
        "# Sometimes it prints out a series of meaningless characters\n",
        "# -> Due to vanishing gradients problem\n",
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "06kb_yXJYFRJ",
        "colab_type": "text"
      },
      "source": [
        "## Text Classifiation with RNNs "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kiD1HZQwmw-o",
        "colab_type": "text"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5Co5nbEHctsd",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow_datasets as tfds\n",
        "import tensorflow as tf\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TMvwMvE8nGCk",
        "colab_type": "text"
      },
      "source": [
        "### Load and preprocess the IMDb dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eUXVl9chc5OM",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# using tfds library, load the imdb dataset\n",
        "dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,\n",
        "                          as_supervised=True)\n",
        "train_dataset, test_dataset = dataset['train'], dataset['test']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WDhdFAVtdGkY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# info object has the lookup table(encoder) of token and index\n",
        "encoder = info.features['text'].encoder\n",
        "print('Vocabulary size: {}'.format(encoder.vocab_size))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "WtlZw3nYd0h6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the encoder\n",
        "sample_string = 'Hello TensorFlow.'\n",
        "\n",
        "encoded_string = encoder.encode(sample_string)\n",
        "print('Encoded string is {}'.format(encoded_string))\n",
        "\n",
        "original_string = encoder.decode(encoded_string)\n",
        "print('The original string: \"{}\"'.format(original_string))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "njJEwjQHd-W2",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Test the encoder\n",
        "for index in encoded_string:\n",
        "  print('{} ----> {}'.format(index, encoder.decode([index])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V0MGZNNieA61",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Creating training task\n",
        "BUFFER_SIZE = 10000\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "train_dataset = train_dataset.shuffle(BUFFER_SIZE)\n",
        "train_dataset = train_dataset.padded_batch(BATCH_SIZE)\n",
        "\n",
        "test_dataset = test_dataset.padded_batch(BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uDEHvQ3ueB8e",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Check the shape of batches\n",
        "for x, y in train_dataset.take(2):\n",
        "    print(x)\n",
        "    print(y)\n",
        "    print(\"-\"*90)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-D3TEHd-pmn8",
        "colab_type": "text"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eM3Zdh79eHKr",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model = tf.keras.Sequential([\n",
        "    tf.keras.layers.Embedding(encoder.vocab_size, 64),\n",
        "    tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),\n",
        "    tf.keras.layers.Dense(64, activation='relu'),\n",
        "    tf.keras.layers.Dense(1)\n",
        "])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oXlLSgeu0Z7Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),\n",
        "              optimizer=tf.keras.optimizers.Adam(1e-4),\n",
        "              metrics=['accuracy'])\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YsVlLGlCpo_o",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "X1TxzddMeJ8R",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "history = model.fit(train_dataset, epochs=5,\n",
        "                    validation_data=test_dataset, \n",
        "                    validation_steps=30)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Tkx_4t1aq1BH",
        "colab_type": "text"
      },
      "source": [
        "### Evalutate the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1akGAPUhIEu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "test_loss, test_acc = model.evaluate(test_dataset)\n",
        "\n",
        "print('Test Loss: {}'.format(test_loss))\n",
        "print('Test Accuracy: {}'.format(test_acc))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K1XKq2DKnBlq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# helper function \n",
        "def plot_graphs(history, metric):\n",
        "  plt.plot(history.history[metric])\n",
        "  plt.plot(history.history['val_'+metric], '')\n",
        "  plt.xlabel(\"Epochs\")\n",
        "  plt.ylabel(metric)\n",
        "  plt.legend([metric, 'val_'+metric])\n",
        "  plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SMDsHaHrmhqO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_graphs(history, 'accuracy')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sW28SoXAmi33",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "plot_graphs(history, 'loss')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s5iyTe_ks2AQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sample_pred(text, model):\n",
        "    encoded_text = encoder.encode(text)\n",
        "    encoded_text = tf.cast(encoded_text, tf.float32)\n",
        "    predictions = model.predict(tf.expand_dims(encoded_text, 0))\n",
        "    prob = tf.sigmoid(predictions)[0][0].numpy()\n",
        "    print('Prob : ', prob)\n",
        "    if prob >= 0.5:\n",
        "        return \"Positive\"\n",
        "    else:\n",
        "        return \"Negative\""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uiI8nDsryxm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "sample_pred_text = 'You should watch this movie, this movie is excellent'\n",
        "sample_pred(sample_pred_text, model)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lsg8Nx1hU6ku",
        "colab_type": "text"
      },
      "source": [
        "# Quiz 1 : Image Classification Model on the CIFAR-10\n",
        "- Build the Convolutional Neural Networks\n",
        "    - Build the model following the bellow model summary\n",
        "    - Apply the dropout regularization to the model and compare the result\n",
        "- Compare the performance of the model built last week\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0rmtvrC6Ug2P",
        "colab_type": "text"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s2ryFJ7GRZew",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "from tensorflow.keras import datasets, models\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "olux865vUkeq",
        "colab_type": "text"
      },
      "source": [
        "### Load the CIFAR-10 Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vcGNnJw5RgJ6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "(train_images, train_labels), (test_images, test_labels) = datasets.cifar10.load_data()\n",
        "\n",
        "# Normalize pixel values to be between 0 and 1\n",
        "train_images, test_images = train_images / 255.0, test_images / 255.0"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vZUrjDtRRhbq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class_names = ['airplane', 'automobile', 'bird', 'cat', 'deer',\n",
        "               'dog', 'frog', 'horse', 'ship', 'truck']\n",
        "\n",
        "plt.figure(figsize=(10,10))\n",
        "for i in range(25):\n",
        "    plt.subplot(5,5,i+1)\n",
        "    plt.xticks([])\n",
        "    plt.yticks([])\n",
        "    plt.grid(False)\n",
        "    plt.imshow(train_images[i], cmap=plt.cm.binary)\n",
        "    # The CIFAR labels happen to be arrays, \n",
        "    # which is why you need the extra index\n",
        "    plt.xlabel(class_names[train_labels[i][0]])\n",
        "plt.show()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n3i1E2yuUphN",
        "colab_type": "text"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fi3F21v8Rlwf",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "layers = tf.keras.layers\n",
        "\n",
        "cifar_model = tf.keras.Sequential([\n",
        "            layers.Conv2D(32, kernel_size=(3, 3), padding = \"same\",\n",
        "                          input_shape = (32, 32, 3), \n",
        "                                activation=\"relu\"),\n",
        "            layers.MaxPool2D(pool_size=2),\n",
        "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPool2D(pool_size=2),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(128, activation=\"relu\"),\n",
        "            layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bSQ8Kin7TSdF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ln_sV_9lUyGZ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model(set optimizer, loss function and metrics)\n",
        "cifar_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q1aZ68EkU5EJ",
        "colab_type": "text"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fu_dXxCvTexl",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_history = cifar_model.fit(train_images, train_labels, epochs=20, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bCy9Bm4cU6sG",
        "colab_type": "text"
      },
      "source": [
        "### Apply the dropout to the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LiUGqcp3VIsF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_drop_model = tf.keras.Sequential([\n",
        "            layers.Conv2D(32, kernel_size=(3, 3), padding = \"same\",\n",
        "                          input_shape = (32, 32, 3), \n",
        "                                activation=\"relu\"),\n",
        "            layers.MaxPool2D(pool_size=2),\n",
        "            layers.Conv2D(64, kernel_size=(3, 3), activation=\"relu\"),\n",
        "            layers.MaxPool2D(pool_size=2),\n",
        "            layers.Flatten(),\n",
        "            layers.Dense(128, activation=\"relu\"),\n",
        "            layers.Dropout(0.5),\n",
        "            layers.Dense(10, activation=\"softmax\")\n",
        "])\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bQctkOHRVcIJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_drop_model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQT4dVNsVnmL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Compile the model(set optimizer, loss function and metrics)\n",
        "cifar_drop_model.compile(optimizer='adam',\n",
        "              loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
        "              metrics=['accuracy'])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8JZ-QEi8VgE4",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "cifar_drop_history = cifar_drop_model.fit(train_images, train_labels, epochs=20, \n",
        "                    validation_data=(test_images, test_labels))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_PAxtOy5V504",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(histories, key='loss'):\n",
        "    plt.figure(figsize=(16,10))\n",
        "\n",
        "    for name, history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                       '--', label=name.title()+' Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "                 label=name.title()+' Train')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(key.replace('_',' ').title())\n",
        "    plt.legend()\n",
        "    plt.xlim([0,max(history.epoch)])\n",
        "\n",
        "plot_history([('Base CNNs', cifar_history),\n",
        "              ('Dropout CNNs', cifar_drop_history)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ovC7aP0nXwst",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def plot_history(histories, key='accuracy'):\n",
        "    plt.figure(figsize=(16,10))\n",
        "\n",
        "    for name, history in histories:\n",
        "        val = plt.plot(history.epoch, history.history['val_'+key],\n",
        "                       '--', label=name.title()+' Val')\n",
        "        plt.plot(history.epoch, history.history[key], color=val[0].get_color(),\n",
        "                 label=name.title()+' Train')\n",
        "\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel(key.replace('_',' ').title())\n",
        "    plt.legend()\n",
        "    plt.xlim([0,max(history.epoch)])\n",
        "\n",
        "plot_history([('Base CNNs', cifar_history),\n",
        "              ('Dropout CNNs', cifar_drop_history)])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AjXv3VaVkz-4",
        "colab_type": "text"
      },
      "source": [
        "# Quiz 2 : Character-level Language Model\n",
        "- Build the Character-level Language Model with LSTM\n",
        "- Compare the generated text to the one generated by RNNs\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "0BYk_VFntIEh"
      },
      "source": [
        "### Import TensorFlow and other libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sHAE5za8tIEj",
        "colab": {}
      },
      "source": [
        "import tensorflow as tf\n",
        "import numpy as np\n",
        "import os\n",
        "import time"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "Fazmd9tKZeCk"
      },
      "source": [
        "### Load and preprocess the Shakespeare dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "uQzk7Pn7ZeCn",
        "colab": {}
      },
      "source": [
        "# 1. Download the Shakespeare's Sonnet dataset\n",
        "path_to_file = tf.keras.utils.get_file('shakespeare.txt',\n",
        "'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')\n",
        "\n",
        "# Load whole text file as a string, then decode.\n",
        "text = open(path_to_file, 'rb').read().decode(encoding='utf-8')\n",
        "\n",
        "# length of text is the number of characters in it\n",
        "print ('Length of text: {} characters'.format(len(text)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "HP7DiATTZeCw",
        "colab": {}
      },
      "source": [
        "# Take a look first 250 characters\n",
        "print(text[:250])\n",
        "\n",
        "# The unique characters in the file\n",
        "vocab = sorted(set(text))\n",
        "print(\"=\"*90)\n",
        "print ('{} unique characters'.format(len(vocab)))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Dqoh-tPvZeC1",
        "colab": {}
      },
      "source": [
        "# 2. Vectorize the text\n",
        "# Creating a mapping from unique characters to indices, and vice versa\n",
        "char2idx = {u:i for i, u in enumerate(vocab)}\n",
        "idx2char = np.array(vocab)\n",
        "\n",
        "# Convert the characters to the indices\n",
        "text_as_int = np.array([char2idx[c] for c in text])\n",
        "\n",
        "# Show how the first 13 characters from the text are mapped to integers\n",
        "print ('{} ---- characters mapped to int ---- > {}'.format(repr(text[:13]), text_as_int[:13]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "E_BnS0bAZeC5",
        "colab": {}
      },
      "source": [
        "# 3. Creating training task\n",
        "# The maximum length sentence we want for a single input in characters\n",
        "seq_length = 100\n",
        "examples_per_epoch = len(text)//(seq_length+1)\n",
        "\n",
        "# Create training examples / targets\n",
        "char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)\n",
        "\n",
        "for i in char_dataset.take(5):\n",
        "  print(idx2char[i.numpy()])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9JyKxrsDZeC9",
        "colab": {}
      },
      "source": [
        "# 'batch' method convert these individual characters to sequences of the desired size\n",
        "sequences = char_dataset.batch(seq_length+1, drop_remainder=True)\n",
        "\n",
        "for item in sequences.take(5):\n",
        "  print(repr(''.join(idx2char[item.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "Sdu54fSrZeDA",
        "colab": {}
      },
      "source": [
        "def split_input_target(chunk):\n",
        "    # input text is shifted to form the target text \n",
        "    input_text = chunk[:-1]\n",
        "    target_text = chunk[1:]\n",
        "    return input_text, target_text\n",
        "\n",
        "# 'map' method lets us easily apply a simple function to each batch\n",
        "dataset = sequences.map(split_input_target)\n",
        "\n",
        "# Print the examples \n",
        "for input_ex, target_ex in  dataset.take(1):\n",
        "  print ('Input : ', repr(''.join(idx2char[input_ex.numpy()])))\n",
        "  print ('Target :', repr(''.join(idx2char[target_ex.numpy()])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "AeNPw4P6ZeDE",
        "colab": {}
      },
      "source": [
        "# 4. Create training batches\n",
        "# Batch size\n",
        "BATCH_SIZE = 64\n",
        "\n",
        "# Buffer size to shuffle the dataset\n",
        "BUFFER_SIZE = 10000\n",
        "\n",
        "dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)\n",
        "\n",
        "dataset"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "WNYE1mCtZeDG"
      },
      "source": [
        "### Build the model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "H3113QVHZeDH"
      },
      "source": [
        "- 3 layers are used to define this model\n",
        "    1. `tf.keras.layers.Embedding`: The input layer, A trainable lookup table that will map the numbers of each character to a vector with `embedding_dim` dimensions;\n",
        "    2. `tf.keras.layers.RNN`: A RNN with size `units=rnn_units`\n",
        "    3. `tf.keras.layers.Dense`: The output layer, with `vocab_size` outputs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "dn_1aNBUZeDH",
        "colab": {}
      },
      "source": [
        "# Length of the vocabulary in chars\n",
        "vocab_size = len(vocab)\n",
        "\n",
        "# The embedding dimension\n",
        "embedding_dim = 256\n",
        "\n",
        "# Number of RNN units\n",
        "rnn_units = 1024"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "wjBCxNttZeDJ",
        "colab": {}
      },
      "source": [
        "def build_model(vocab_size, embedding_dim, rnn_units, batch_size):\n",
        "    layers = tf.keras.layers\n",
        "    model = tf.keras.Sequential([\n",
        "            layers.Embedding(input_dim=vocab_size,\n",
        "                            output_dim=embedding_dim,\n",
        "                            batch_input_shape=[batch_size, None]\n",
        "                            ),\n",
        "            layers.LSTM(rnn_units, # you can change the RNN type to LSTM or GRU\n",
        "                            return_sequences=True,\n",
        "                            stateful=True,\n",
        "                            recurrent_initializer='glorot_uniform' # Xavier Initialization\n",
        "                            ),\n",
        "            layers.Dense(vocab_size)\n",
        "    ])\n",
        "    return model"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "8mri-YydZeDL",
        "colab": {}
      },
      "source": [
        "model = build_model(\n",
        "  vocab_size = vocab_size,\n",
        "  embedding_dim=embedding_dim,\n",
        "  rnn_units=rnn_units,\n",
        "  batch_size=BATCH_SIZE)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "mecE1AmqZeDN"
      },
      "source": [
        "### Try the model\n",
        "Before training the model, take a look about how does the model works"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "DbL8NHCTZeDN",
        "colab": {}
      },
      "source": [
        "# Check the shape of the output\n",
        "for input_example_batch, target_example_batch in dataset.take(1):\n",
        "  example_batch_predictions = model(input_example_batch)\n",
        "  print(example_batch_predictions.shape, \"# (batch_size, sequence_length, vocab_size)\")"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "cg9Ew5SjZeDP",
        "colab": {}
      },
      "source": [
        "# Check the model architecture\n",
        "# Model can be run on inputs of any length \n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "hCRnjLJnZeDS",
        "colab": {}
      },
      "source": [
        "# We need to sample from the output distribution, not to take the argmax of the distribution\n",
        "sampled_indices = tf.random.categorical(example_batch_predictions[0], num_samples=1)\n",
        "sampled_indices = tf.squeeze(sampled_indices,axis=-1).numpy()\n",
        "\n",
        "# Predictions of the next character index \n",
        "sampled_indices"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "sQX4wfWLZeDU",
        "colab": {}
      },
      "source": [
        "# Decode the predictions, the model shows poor performance \n",
        "print(\"Input: \\n\", repr(\"\".join(idx2char[input_example_batch[0]])))\n",
        "print()\n",
        "print(\"Next Char Predictions: \\n\", repr(\"\".join(idx2char[sampled_indices ])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "ooQq6oHhZeDV"
      },
      "source": [
        "### Train the model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "kDvxH_0mZeDW",
        "colab": {}
      },
      "source": [
        "# 1. Attach an optimizer, and a loss function\n",
        "# define the loss function \n",
        "def loss(labels, logits):\n",
        "    # Because the output of model is logit, \n",
        "    return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)\n",
        "\n",
        "# Test the loss function\n",
        "example_batch_loss  = loss(target_example_batch, example_batch_predictions)\n",
        "print(\"Prediction shape: \", example_batch_predictions.shape, \" # (batch_size, sequence_length, vocab_size)\")\n",
        "print(\"scalar_loss:      \", example_batch_loss.numpy().mean())\n",
        "\n",
        "# Configure the training procedure\n",
        "optimizer = tf.keras.optimizers.Adam(clipnorm=5.0)\n",
        "model.compile(optimizer=optimizer, loss=loss)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "3BkbbnY0ZeDX",
        "colab": {}
      },
      "source": [
        "# 2. Configure the checkpoints\n",
        "# `tf.keras.callbacks.ModelCheckpoint` : The callback function to save the model checkpoint\n",
        "\n",
        "# Directory where the model weights will be saved\n",
        "ckpt_dir = './training_lstm_ckpts'\n",
        "\n",
        "# Checkpoint name\n",
        "ckpt_prefix = os.path.join(ckpt_dir, \"ckpt_lstm_{epoch}\")\n",
        "\n",
        "# Callback function to save the model weights\n",
        "ckpt_callback=tf.keras.callbacks.ModelCheckpoint(\n",
        "    filepath=ckpt_prefix,\n",
        "    save_weights_only=True)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "YwNF7QIIZeDZ",
        "colab": {}
      },
      "source": [
        "# 3. Execute the training\n",
        "EPOCHS=10\n",
        "rnn_history = model.fit(dataset, epochs=EPOCHS, callbacks=[ckpt_callback])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "6VydZLxnZeDa"
      },
      "source": [
        "### Generate text"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "v3aEFsrHZeDa",
        "colab": {}
      },
      "source": [
        "# Check the latest checkpoint\n",
        "tf.train.latest_checkpoint(ckpt_dir)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "onrk0QvaZeDc",
        "colab": {}
      },
      "source": [
        "# To run the model with one sample(not with batch_size of samples),\n",
        "# We rebuild the model, and load the weights from the saved checkpoint. \n",
        "model = build_model(vocab_size, embedding_dim, rnn_units, batch_size=1)\n",
        "\n",
        "model.load_weights(tf.train.latest_checkpoint(ckpt_dir))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "19OlMnj4ZeDe",
        "colab": {}
      },
      "source": [
        "# Check the model summary\n",
        "model.summary()"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "aBqBnC-GZeDf",
        "colab": {}
      },
      "source": [
        "# The prediction loop\n",
        "def generate_text(model, start_string):\n",
        "  # Number of characters to generate\n",
        "  n_generate = 1000\n",
        "\n",
        "  # Converting start_strings to index (vectorizing)\n",
        "  input_eval = [char2idx[s] for s in start_string]\n",
        "  input_eval = tf.expand_dims(input_eval, 0)\n",
        "\n",
        "  # Making the empty list to store results\n",
        "  text_generated = []\n",
        "\n",
        "  # Low temperatures -> more predictable text.\n",
        "  # Higher temperatures -> more surprising text.\n",
        "  temperature = 1\n",
        "\n",
        "  # Here batch size == 1\n",
        "  model.reset_states()\n",
        "  for i in range(n_generate):\n",
        "      predictions = model(input_eval)\n",
        "      # remove the batch dimension\n",
        "      predictions = tf.squeeze(predictions, 0)\n",
        "\n",
        "      # using a categorical distribution to predict the character\n",
        "      predictions = predictions / temperature\n",
        "      predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()\n",
        "\n",
        "      # Passing the predicted character as the next input to the model\n",
        "      # along with the previous hidden state\n",
        "      input_eval = tf.expand_dims([predicted_id], 0)\n",
        "\n",
        "      text_generated.append(idx2char[predicted_id])\n",
        "\n",
        "  return (start_string + ''.join(text_generated))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "9zuUvXb-ZeDg",
        "colab": {}
      },
      "source": [
        "# It shows poor performance \n",
        "# Sometimes it prints out a series of meaningless characters\n",
        "# -> Due to vanishing gradients problem\n",
        "print(generate_text(model, start_string=u\"ROMEO: \"))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}